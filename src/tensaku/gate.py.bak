"""
@module: gate
@role: devで温度Tを推定→ MSP/entropy/Trust を算出→ CSE(|pred-true|>=c)≤ε を満たす最大coverageのτ探索→ pool判定/擬似ラベル出力
@inputs: {data_dir}/labeled.jsonl, {data_dir}/dev.jsonl, {data_dir}/pool.jsonl, {out_dir}/checkpoints_min/best.pt
@outputs: {out_dir}/gate_decision.csv（id,y_pred,conf_msp,entropy,trust,decision_epsXX,pseudo_label）, {out_dir}/gate_meta.json
@cli: tensaku gate
@notes: collate安定化のため未採点の y=-1。温度は dev の NLL 最小で推定し全セットに適用。
"""

# Path: /home/esakit25/work/tensaku/src/tensaku/gate.py
from __future__ import annotations
import os, json, math, csv
from typing import Dict, Any, List, Tuple, Optional
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.neighbors import NearestNeighbors

# ---------- IO ----------
def _read_jsonl(path: str) -> List[dict]:
    rows=[]
    if not os.path.exists(path): return rows
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line=line.strip()
            if not line: continue
            try: rows.append(json.loads(line))
            except Exception: pass
    return rows

# ---------- Dataset ----------
def _make_loader(rows: List[dict], tok, text_key: str, max_len: int, bs: int) -> DataLoader:
    class DS(Dataset):
        def __init__(self, rows): self.rows=rows
        def __len__(self): return len(self.rows)
        def __getitem__(self, i):
            r = self.rows[i]
            txt = r.get(text_key, "")
            if isinstance(txt, list): txt = " ".join(map(str, txt))
            enc = tok(txt, truncation=True, padding="max_length", max_length=max_len, return_tensors="pt")
            item = {k:v.squeeze(0) for k,v in enc.items()}
            item["id"] = r.get("id", str(i))
            y = r.get("score", None)
            # ★ collateエラー回避：常に LongTensor を返す（未採点は -1）
            item["y"] = torch.tensor(int(y), dtype=torch.long) if y is not None else torch.tensor(-1, dtype=torch.long)
            return item
    return DataLoader(DS(rows), batch_size=bs, shuffle=False, num_workers=0)

# ---------- Utils ----------
def _softmax(x: torch.Tensor, dim=-1) -> torch.Tensor:
    return torch.softmax(x, dim=dim)

def _entropy_from_probs(p: np.ndarray, eps: float=1e-12) -> np.ndarray:
    p = np.clip(p, eps, 1.0)
    return (-p * np.log(p)).sum(axis=1)

def _temperature_search(logits_dev: np.ndarray, y_dev: np.ndarray, bins:int=15) -> float:
    """T ∈ [0.5,3.0] を0.05刻みで探索（NLL最小）。"""
    T_best, best_nll = 1.0, 1e18
    for T in np.arange(0.5, 3.01, 0.05):
        z = logits_dev / T
        z = z - z.max(axis=1, keepdims=True)
        p = np.exp(z); p = p / p.sum(axis=1, keepdims=True)
        nll = -np.log(np.clip(p[(np.arange(len(y_dev)), y_dev)], 1e-12, 1.0)).mean()
        if nll < best_nll:
            best_nll, T_best = nll, float(T)
    return T_best

def _compute_trust_cosine(
    X_train: np.ndarray, y_train: np.ndarray, X_q: np.ndarray, y_pred_q: np.ndarray, k:int=1
) -> np.ndarray:
    """Trust Score (cosine)。"""
    n = len(X_q); trust = np.zeros(n, dtype=np.float32)
    classes = np.unique(y_train)
    nn_same, nn_other = {}, {}
    for c in classes:
        idx_s = np.where(y_train == c)[0]; idx_o = np.where(y_train != c)[0]
        if len(idx_s)==0 or len(idx_o)==0: continue
        nn_same[c]  = (NearestNeighbors(n_neighbors=min(k,len(idx_s)), metric="cosine").fit(X_train[idx_s]), idx_s)
        nn_other[c] = (NearestNeighbors(n_neighbors=min(k,len(idx_o)), metric="cosine").fit(X_train[idx_o]), idx_o)
    for i in range(n):
        c = y_pred_q[i]
        if c in nn_same and c in nn_other:
            nn_s,_ = nn_same[c]; nn_o,_ = nn_other[c]
            ds,_ = nn_s.kneighbors(X_q[i:i+1], return_distance=True)
            do,_ = nn_o.kneighbors(X_q[i:i+1], return_distance=True)
            dp = float(ds.mean()); dc = float(do.mean())
            trust[i] = dc / (dp + dc + 1e-12)
        else:
            trust[i] = 0.0
    return trust

def _infer_num_labels_from_ckpt_or_data(sd: dict, model_cfg: dict, data_dir: str, files: dict, label_key: str) -> int:
    n = sd.get("n_class", None)
    if isinstance(n,(int,float)): return int(n)
    state = sd.get("model", {})
    if isinstance(state, dict):
        w = state.get("classifier.weight", None)
        if isinstance(w, torch.Tensor) and w.ndim>=2: return int(w.shape[0])
    mx = -1
    for k in ("labeled","dev","test"):
        p = os.path.join(data_dir, files.get(k, f"{k}.jsonl"))
        for r in _read_jsonl(p):
            v = r.get(label_key, None)
            if v is None: continue
            try: mx = max(mx, int(v))
            except Exception: pass
    if mx >= 0: return mx+1
    v = model_cfg.get("num_labels", None)
    if isinstance(v,(int,float)) and int(v)>1: return int(v)
    return 6

# ---------- Main ----------
@torch.no_grad()
def run(cfg: Dict[str,Any]) -> None:
    run = cfg["run"]; data = cfg["data"]; model_cfg = cfg["model"]
    hitl = cfg.get("hitl", {})
    conf_cfg = cfg.get("confidence", {})
    cal_cfg  = conf_cfg.get("calibration", {})

    # Config
    data_dir  = run["data_dir"]; files = data["files"]
    text_key  = data.get("text_key_primary", "mecab")
    max_len   = int(data.get("max_len", 128))
    bs_infer  = int(data.get("batch_size_infer", 32))
    label_key = data.get("label_key", "score")

    cse_abs_err   = int(hitl.get("cse_abs_err", 2))
    eps_list      = [float(x) for x in hitl.get("eps_list", [0.02, 0.05])]
    safety_margin = float(hitl.get("safety_margin", 0.01))
    pseudo_thr    = float(hitl.get("pseudo_label_thresh", 0.97))

    use_temp = bool(cal_cfg.get("enable", True))
    ece_bins = int(cal_cfg.get("ece_bins", 15))

    # 入力データ
    rows_lab  = _read_jsonl(os.path.join(data_dir, files.get("labeled","labeled.jsonl")))
    rows_dev  = _read_jsonl(os.path.join(data_dir, files.get("dev","dev.jsonl")))
    rows_pool = _read_jsonl(os.path.join(data_dir, files.get("pool","pool.jsonl")))
    if not rows_dev:
        print("[gate] empty dev; abort."); return
    if not rows_pool:
        print("[gate] empty pool; run make_pool & infer-pool first."); return

    # モデル
    name = model_cfg.get("name","cl-tohoku/bert-base-japanese-v3")
    tok = AutoTokenizer.from_pretrained(name, use_fast=False)
    ckpt_path = os.path.join(run.get("out_dir","./outputs"), "checkpoints_min", "best.pt")
    if not os.path.exists(ckpt_path):
        print(f"[gate] missing ckpt: {ckpt_path}"); return
    sd = torch.load(ckpt_path, map_location="cpu")
    n_class = _infer_num_labels_from_ckpt_or_data(sd, model_cfg, data_dir, files, label_key)
    model = AutoModelForSequenceClassification.from_pretrained(name, num_labels=n_class)
    model.load_state_dict(sd.get("model", {}), strict=False)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device).eval()

    # 推論関数（logits/probs/CLS埋め込み）
    def infer_rows(rows: List[dict]) -> Tuple[List[str], np.ndarray, np.ndarray, np.ndarray, Optional[np.ndarray]]:
        ids=[]; logits_all=[]; probs_all=[]; feats=[]; ys=[]
        loader = _make_loader(rows, tok, text_key, max_len, bs_infer)
        for batch in loader:
            ids.extend(batch["id"])
            ys.append(batch["y"].cpu().numpy())  # devは>=0、poolは-1
            inputs = {k:(v.to(device) if hasattr(v,"to") else v) for k,v in batch.items() if k not in ("id","y")}
            out = model(**inputs, output_hidden_states=False)
            logits = out.logits; prob = _softmax(logits, dim=-1)
            # CLS埋め込み
            try:
                cls = model.bert(**inputs).last_hidden_state[:,0,:]
            except Exception:
                cls = torch.zeros((logits.size(0), 768), device=device)
            logits_all.append(logits.cpu().numpy())
            probs_all.append(prob.cpu().numpy())
            feats.append(cls.detach().cpu().numpy())
        logits_all = np.concatenate(logits_all, axis=0) if logits_all else np.zeros((0,n_class))
        probs_all  = np.concatenate(probs_all,  axis=0) if probs_all  else np.zeros((0,n_class))
        feats      = np.concatenate(feats,      axis=0) if feats      else np.zeros((0,768))
        y_true     = np.concatenate(ys,         axis=0) if ys         else None
        return ids, logits_all, probs_all, feats, y_true

    ids_lab, logits_lab, probs_lab, emb_lab, y_lab = infer_rows(rows_lab) if rows_lab else ([],np.zeros((0,n_class)),np.zeros((0,n_class)),np.zeros((0,768)),None)
    ids_dev, logits_dev, probs_dev, emb_dev, y_dev = infer_rows(rows_dev)
    ids_pool, logits_pool, probs_pool, emb_pool, y_pool = infer_rows(rows_pool)

    # 温度スケーリング（devでT推定→全体に適用）
    T = 1.0
    if use_temp and len(probs_dev) > 0 and y_dev is not None:
        # devは教師ありのはず（全て >=0 を想定）
        if (y_dev < 0).any():
            y_dev = y_dev[y_dev >= 0]
            logits_dev = logits_dev[:len(y_dev)]
            probs_dev = probs_dev[:len(y_dev)]
        y_dev = y_dev.astype(int)
        T = _temperature_search(logits_dev, y_dev, bins=ece_bins)
        def apply_T(logits: np.ndarray) -> np.ndarray:
            z = logits / T; z = z - z.max(axis=1, keepdims=True)
            p = np.exp(z); p = p / p.sum(axis=1, keep_dims=True) if hasattr(np, "keep_dims") else p / p.sum(axis=1, keepdims=True)
            return p
        if len(probs_lab)>0: probs_lab = apply_T(logits_lab)
        probs_dev = apply_T(logits_dev)
        probs_pool= apply_T(logits_pool)

    # 確信度（MSP/entropy）
    conf_msp_dev = probs_dev.max(axis=1)
    pred_dev     = probs_dev.argmax(axis=1)
    ent_dev      = _entropy_from_probs(probs_dev)
    y_dev_arr    = y_dev.astype(int)

    # Trust Score（cosine, k=1）
    trust_dev = np.zeros_like(conf_msp_dev)
    trust_pool= np.zeros(len(ids_pool), dtype=np.float32)
    if len(emb_lab) > 0 and y_lab is not None:
        y_lab_arr = y_lab[y_lab >= 0].astype(int) if (y_lab < 0).any() else y_lab.astype(int)
        emb_lab_eff = emb_lab[: len(y_lab_arr)]  # 念のため長さを合わせる
        trust_dev  = _compute_trust_cosine(emb_lab_eff, y_lab_arr, emb_dev,  pred_dev, k=1)
        pred_pool  = probs_pool.argmax(axis=1)
        trust_pool = _compute_trust_cosine(emb_lab_eff, y_lab_arr, emb_pool, pred_pool, k=1)

    # τ探索（dev; MSPベース）：CSE(|pred-true|>=c) ≤ ε - margin を満たす coverage 最大の τ
    def find_tau_for_eps(conf: np.ndarray, y: np.ndarray, pred: np.ndarray, eps: float) -> Tuple[float, float]:
        taus = np.quantile(conf, np.linspace(0.0,1.0,201))
        abs_err = np.abs(pred - y); cse = (abs_err >= cse_abs_err).astype(np.float32)
        order = np.argsort(-conf); conf_s = conf[order]; cse_s = cse[order]
        prefix_cse = np.cumsum(cse_s)
        best_cov, best_tau = 0.0, None
        for tau in taus:
            k = int((conf_s >= tau).sum())
            if k == 0: continue
            cse_rate = float(prefix_cse[k-1] / k)
            if cse_rate <= max(eps - safety_margin, 0.0):
                cov = k / len(conf_s)
                if cov > best_cov or (math.isclose(cov, best_cov) and (best_tau is None or tau > best_tau)):
                    best_cov, best_tau = cov, float(tau)
        if best_tau is None:
            best_tau = float(taus.max()); best_cov = 0.0
        return best_tau, best_cov

    tau_by_eps = {}
    for eps in [float(e) for e in cfg.get("hitl", {}).get("eps_list", [0.02, 0.05])]:
        tau, cov = find_tau_for_eps(conf_msp_dev, y_dev_arr, pred_dev, float(eps))
        tau_by_eps[float(eps)] = (tau, cov)

    # pool へ適用 & 出力
    out_dir = run.get("out_dir","./outputs"); os.makedirs(out_dir, exist_ok=True)
    out_gate = os.path.join(out_dir, "gate_decision.csv")
    with open(out_gate, "w", encoding="utf-8", newline="") as w:
        fieldnames = ["id","y_pred","conf_msp","entropy","trust"] + [f"decision_eps{int(eps*100)}" for eps in cfg.get("hitl", {}).get("eps_list", [0.02,0.05])] + ["pseudo_label"]
        cw = csv.DictWriter(w, fieldnames=fieldnames); cw.writeheader()
        pred_pool = probs_pool.argmax(axis=1)
        conf_msp_pool = probs_pool.max(axis=1)
        ent_pool = _entropy_from_probs(probs_pool)
        for i, pid in enumerate(ids_pool):
            row = {
                "id": pid,
                "y_pred": int(pred_pool[i]),
                "conf_msp": f"{conf_msp_pool[i]:.6f}",
                "entropy": f"{ent_pool[i]:.6f}",
                "trust": f"{float(trust_pool[i]):.6f}",
                "pseudo_label": "yes" if conf_msp_pool[i] >= max(pseudo_thr, 0.0) else "no",
            }
            for eps in cfg.get("hitl", {}).get("eps_list", [0.02,0.05]):
                tau,_ = tau_by_eps[float(eps)]
                row[f"decision_eps{int(eps*100)}"] = "accept" if conf_msp_pool[i] >= tau else "hold"
            cw.writerow(row)

    out_meta = os.path.join(out_dir, "gate_meta.json")
    with open(out_meta, "w", encoding="utf-8") as f:
        json.dump({
            "n_class": int(n_class),
            "temperature": float(T),
            "eps_list": cfg.get("hitl", {}).get("eps_list", [0.02,0.05]),
            "cse_abs_err": int(cse_abs_err),
            "safety_margin": float(safety_margin),
            "pseudo_label_thresh": float(pseudo_thr),
            "tau_by_eps": {str(k): {"tau": float(v[0]), "coverage_dev": float(v[1])} for k,v in tau_by_eps.items()},
            "dev_size": len(rows_dev),
            "pool_size": len(rows_pool),
        }, f, ensure_ascii=False, indent=2)

    print(f"[gate] wrote {out_gate}")
    print("".join([
        "[gate] thresholds (MSP): ",
        ", ".join([f"ε={eps:.2%} → τ={tau_by_eps[eps][0]:.4f} (cov={tau_by_eps[eps][1]:.1%})" for eps in cfg.get("hitl", {}).get("eps_list", [0.02,0.05])])
    ]))
